# Naive Bayes Archetype Classification (NBAC)

## Introduction

The Naive Bayes Archetype Classification (NBAC) is a probabilistic model used to
identify the most likely archetypes for a given decklist based on observed card
counts from a labeled corpus of decks. NBAC treats a decklist as a bag of card draws
and computes the posterior probability of each archetype using a smoothed
multinomial Naive Bayes model.

This document explains how NBAC is trained, how inference is performed, and how
we improve robustness in the presence of imperfect (noisy) archetype labels.

## Definitions

- **Vocabulary ($V$)**: The set of all unique card names observed in the training

  corpus for a given format. We denote its size as $|V|$.
- **Decklist ($D$)**: A multiset of cards in a deck where each card $c \in V$ has

  an associated non-negative integer count $k_c \in \{0,1,2,3,4\}$.
- **Archetype ($A$)**: A discrete class label representing an archetype within a

  format. Let $\mathcal{A}$ be the set of all archetypes.
- **Training Corpus ($\mathcal{T}$)**: A collection of labeled decklists

  $\{(D_i, y_i)\}_{i=1}^N$, where $y_i \in \mathcal{A}$.
- **Card Count Mass**: For any decklist $D$, $\|D\|_1 = \sum_{c \in V} k_c$.
- **Global Background Distribution ($q$)**: A probability distribution over cards

  computed from the entire corpus for a format.

## Model

NBAC uses the multinomial Naive Bayes assumption: conditioned on an archetype
$A$, the decklist is generated by drawing cards independently from a categorical
distribution $\theta_A$ over the vocabulary.

### Class Prior

The archetype prior is defined as:

$$
P(A) = \frac{N_A}{N}
$$

where $N_A$ is the number of labeled decks in the corpus with label $A$, and
$N$ is the total number of labeled decks in the corpus.

### Archetype Card Distribution

For each archetype $A$ and card $c$, define the observed corpus count:

$$
n_{A,c} = \sum_{\substack{(D_i, y_i) \in \mathcal{T} \\ y_i = A}} k_{i,c}
$$

and the total mass for archetype $A$:

$$
N^{\text{mass}}_A = \sum_{c \in V} n_{A,c}
$$

We compute a Dirichlet-smoothed estimate of the per-card probability:

$$

  heta_{A,c} = P(c \mid A) = \frac{n_{A,c} + \alpha}{N^{\text{mass}}_A + \alpha |V|}
$$

where $\alpha > 0$ is a smoothing parameter.

## Inference

Given a decklist $D$, NBAC computes unnormalized log-scores for each archetype:

$$
\log s(A \mid D) = \log P(A) + \sum_{\substack{c \in V \\ k_c > 0}} k_c \log \theta_{A,c}
$$

The posterior probability of archetype $A$ is then:

$$
P(A \mid D) = \frac{\exp(\log s(A \mid D))}{\sum\limits_{A' \in \mathcal{A}} \exp(\log s(A' \mid D))}
$$

In practice, the summation is computed only over cards present in the decklist.

## Background Interpolation

Due to noisy labels and overlapping archetypes, it is beneficial to smooth each
archetype distribution toward a global (format-wide) background distribution.

We define the background corpus counts as:

$$
n_{\text{all},c} = \sum_{(D_i, y_i) \in \mathcal{T}} k_{i,c}
$$

and total mass across the full corpus:

$$
N^{\text{mass}}_{\text{all}} = \sum_{c \in V} n_{\text{all},c}
$$

A background distribution is then computed as:

$$
q_c = \frac{n_{\text{all},c} + \alpha}{N^{\text{mass}}_{\text{all}} + \alpha |V|}
$$

We interpolate each archetype distribution with the background distribution:

$$

  heta'_{A,c} = (1 - \lambda)\theta_{A,c} + \lambda q_c
$$

where $\lambda \in [0,1]$ controls the strength of background interpolation.

When background interpolation is enabled, $\theta'_{A,c}$ replaces $\theta_{A,c}$
in the inference equations.

## One-Pass Self-Filtering

To reduce sensitivity to mislabeled or outlier decks, NBAC supports a one-pass
self-filtering step.

1. Train an initial model using the corpus $\mathcal{T}$.
2. For each labeled example $(D_i, y_i)$, compute the posterior $P(y_i \mid D_i)$.
3. For each archetype $A$, rank all decks labeled $A$ by $P(A \mid D)$.
4. Remove or downweight the bottom fraction $\rho$ of decks for each archetype.

A retrained model is then fit on the filtered (or reweighted) corpus.

## Calibration

Naive Bayes posteriors are often poorly calibrated. NBAC supports temperature
scaling to improve probability calibration without changing the underlying
ranking significantly.

Let $\log s(A \mid D)$ be the unnormalized log-score. The calibrated posterior is:

$$
P_T(A \mid D) = \text{softmax}\left(\frac{\log s(A \mid D)}{T}\right)
$$

where $T > 0$ is chosen to minimize negative log-likelihood on a held-out
validation set.

## Ambiguity Handling

When archetypes overlap, a single deck may be consistent with multiple labels.
NBAC supports an ambiguity policy based on the top-1 and top-2 posteriors.

Let $p_1$ and $p_2$ denote the highest and second-highest posterior probabilities,
respectively. The classification may be treated as undecisive if:

- $p_1 < p_{\min}$, or
- $p_1 - p_2 < \delta$

where $p_{\min}$ and $\delta$ are configurable thresholds.

## Interpretability

For an archetype $A$ and decklist $D$, the per-card contribution to the
unnormalized log-score is:

$$
  ext{contrib}(c; A, D) = k_c \log \theta_{A,c}
$$

(or $k_c \log \theta'_{A,c}$ when background interpolation is enabled).

For better interpretability, we can also define a background-relative lift score:

$$
  ext{lift}(c; A, D) = k_c \left(\log \theta'_{A,c} - \log q_c\right)
$$

Cards with the largest positive lift are the strongest pieces of evidence for
archetype $A$ in the given decklist.

## Notes

- Card quantities are incorporated directly via counts $k_c$.
- The model is trained per format.
- This specification does not prescribe a particular storage format; the worker
  stores per-card log-probabilities in a compact binary format in D1.
